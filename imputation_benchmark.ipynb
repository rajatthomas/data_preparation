{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "\n",
    "#from modshogun import *\n",
    "\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, LeaveOneGroupOut\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, Imputer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from CustomCVs import KFoldMixedSizes, StratifiedKFoldMixedSizes, StratifiedKFoldByGroups\n",
    "#from evaluation_classifier import Evaluater\n",
    "\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute, IterativeSVD #, MICE\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, columns, complete=False):\n",
    "    if complete:\n",
    "        tmp_df = df[~df.loc[:, np.append(\"Dx\", columns)].T.isnull().any()]\n",
    "    else:\n",
    "        tmp_df = df[~df.loc[:, [\"Dx\"]].T.isnull().any()]\n",
    "\n",
    "# remove subjects whom miss over 10% of their features\n",
    "\n",
    "#         nans_per_subject = tmp_df.loc[:, tmp_df.columns.values[:]].isnull().T.sum()\n",
    "#         completeness_per_subject = (float(tmp_df.shape[1]) - nans_per_subject) / tmp_df.shape[1]\n",
    "#         tmp_df = tmp_df.loc[completeness_per_subject >= 0.9]\n",
    "        \n",
    "    X = tmp_df.loc[:, columns].as_matrix()\n",
    "    y = tmp_df.loc[:, \"Dx\"].values.astype(int)\n",
    "\n",
    "    tmp_df['age_group_tesla_site'] = tmp_df[\"age_group\"] + tmp_df[\"tesla\"] + tmp_df[\"site\"]\n",
    "    le = LabelEncoder()\n",
    "    groups = le.fit_transform(tmp_df.age_group_tesla_site.values)\n",
    "    \n",
    "    return X, y, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'AgeSQ']\n",
      "Clean Laccumb\n",
      "Lamyg R_parsorbitalis_surfavg\n",
      "R_parstriangularis_surfavg R_parsorbitalis_thickavg\n",
      "(4370, 180)\n",
      "(683, 180)\n",
      "(4243, 180)\n",
      "(1450, 180)\n",
      "(2114, 180)\n",
      "(2409, 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/local/softwares/anaconda3/envs/py36ml/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/data/local/softwares/anaconda3/envs/py36ml/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"/data/rmthomas/HeteroSmallSample\"\n",
    "ENIGMA_OCD_df = pd.read_csv(os.path.join(data_dir, \"ENIGMA_OCD.csv\"))\n",
    "ENIGMA_OCD_df.head()\n",
    "\n",
    "# Specify columns of interest for covariates and FS features\n",
    "covariates_columns = [ENIGMA_OCD_df.columns.values[3], ENIGMA_OCD_df.columns.values[6]]\n",
    "print(covariates_columns)\n",
    "subcort_columns = ENIGMA_OCD_df.columns.values[9:9+16]\n",
    "print(subcort_columns[0], subcort_columns[-1])\n",
    "cort_s_columns = ENIGMA_OCD_df.columns.values[9+16:25+70]\n",
    "print(cort_s_columns[0], cort_s_columns[-1])\n",
    "cort_t_columns = ENIGMA_OCD_df.columns.values[25+70:25+70+70]\n",
    "print(cort_t_columns[0], cort_t_columns[-1])\n",
    "\n",
    "print(ENIGMA_OCD_df.shape)\n",
    "print(ENIGMA_OCD_df[~ENIGMA_OCD_df.T.isnull().any()].shape)\n",
    "print(ENIGMA_OCD_df[~ENIGMA_OCD_df.loc[:, covariates_columns].T.isnull().any()].shape)\n",
    "print(ENIGMA_OCD_df[~ENIGMA_OCD_df.loc[:, subcort_columns].T.isnull().any()].shape)\n",
    "print(ENIGMA_OCD_df[~ENIGMA_OCD_df.loc[:, cort_s_columns].T.isnull().any()].shape)\n",
    "print(ENIGMA_OCD_df[~ENIGMA_OCD_df.loc[:, cort_t_columns].T.isnull().any()].shape)\n",
    "\n",
    "ENIGMA_OCD_df['age_group_tesla_site'] = ENIGMA_OCD_df[\"age_group\"] + ENIGMA_OCD_df[\"tesla\"] + ENIGMA_OCD_df[\"site\"]\n",
    "\n",
    "columns_to_choose = list(range(3, len(ENIGMA_OCD_df.columns) - 1))\n",
    "columns_to_choose.pop(1)\n",
    "columns_to_choose.pop(1)\n",
    "\n",
    "columns_to_choose = ENIGMA_OCD_df.columns.values[columns_to_choose]\n",
    "\n",
    "X_tot, y_tot, groups_tot = create_dataset(ENIGMA_OCD_df, columns_to_choose, complete=False)\n",
    "X_c_tot, y_c_tot, groups_c_tot = create_dataset(ENIGMA_OCD_df, columns_to_choose, complete=True)\n",
    "\n",
    "subcort_idx = np.where(columns_to_choose == \"LLatVent\")[0][0]\n",
    "cort_surf_idx = np.where(columns_to_choose == \"LSurfArea\")[0][0]\n",
    "cort_thick_idx = np.where(columns_to_choose == \"LThickness\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_choose = list(range(3, len(ENIGMA_OCD_df.columns) - 1))\n",
    "print(len(columns_to_choose))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NaNs per column (features)\n",
    "nans_per_column = np.isnan(X_tot).sum(axis=0)\n",
    "\n",
    "# Count NaNs per subjects and select only those who have at least one NaN \n",
    "nans_per_subject = np.isnan(X_tot).sum(axis=1)[np.isnan(X_tot).sum(axis=1) > 0]\n",
    "n_to_pick_from = len(nans_per_subject)\n",
    "\n",
    "# Now from these \"has-NaN\" subjects select the N equals the total \"has-no-NaN\" subjects\n",
    "n_to_choose = X_tot[~np.isnan(X_tot).any(axis=1), :].shape[0]\n",
    "\n",
    "# We pick only those \"has-NaN\" subjects with the highest amount of NaNs\n",
    "sorted_nans_per_subject = nans_per_subject[np.argsort(nans_per_subject)]\n",
    "indices = np.round(np.linspace(0, n_to_pick_from-1, num=n_to_choose))\n",
    "\n",
    "nans_per_subject = sorted_nans_per_subject[np.array(indices, dtype=int)]\n",
    "# nans_per_subject[0] = 0\n",
    "\n",
    "# Here we calculate the NaN probabilites per column and subject\n",
    "p_columns = nans_per_column / float(nans_per_column.sum())\n",
    "p_subjects = nans_per_subject / float(nans_per_subject.sum())\n",
    "\n",
    "# Calculate percentage of missing data in original data set\n",
    "missing_data_percentage =  float(nans_per_column.sum()) /  X_tot.size\n",
    "\n",
    "# Obtain a complete dataset without NaNs that we can use for imputation simulations\n",
    "X_to_impute = X_c_tot.copy()\n",
    "\n",
    "# Create a flat array with matrix indices used for mapping\n",
    "items = np.arange(X_to_impute.size)\n",
    "\n",
    "# Calculate probability matrix for each element\n",
    "prob_matrix = np.reshape(p_columns, (1, p_columns.shape[0])) * np.reshape(p_subjects, (p_subjects.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prob_matrix > 0).sum() / float(prob_matrix.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print p_subjects.max()\n",
    "print p_columns.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_perc_nan_col = (nans_per_column / float(nans_per_column.sum())) * 100\n",
    "\n",
    "true_perc_nan_subc = true_perc_nan_col[:cort_surf_idx].sum()\n",
    "true_perc_nan_cort_s = true_perc_nan_col[cort_surf_idx:cort_thick_idx].sum()\n",
    "true_perc_nan_cort_t = true_perc_nan_col[cort_thick_idx:].sum()\n",
    "\n",
    "print true_perc_nan_subc\n",
    "print true_perc_nan_cort_s\n",
    "print true_perc_nan_cort_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_nans[~np.isnan(X_with_nans).any(axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_regression(X_with_nans, X_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_regression(X_train, X_test):\n",
    "    \n",
    "    # Check if training data contains any columns which only contain NaNs \n",
    "    X_train_NaN_only_column_idx = np.where(np.isnan(X_train).sum(axis=0) == X_train.shape[0])[0]\n",
    "    \n",
    "    if len(X_train_NaN_only_column_idx) > 0:    \n",
    "        X_train_imputed = np.delete(X_train, X_train_NaN_only_column_idx, 1)\n",
    "        X_test_imputed = np.delete(X_test, X_train_NaN_only_column_idx, 1)\n",
    "    else:\n",
    "        X_train_imputed, X_test_imputed = X_train.copy(), X_test.copy()\n",
    "\n",
    "    n_subjects, n_features = X_train_imputed.shape\n",
    "    X_train_no_nans = X_train_imputed[~np.isnan(X_train_imputed).any(axis=1), :]\n",
    "    \n",
    "    if X_train_no_nans.shape[0] <= 1:\n",
    "        return None\n",
    "    \n",
    "    corr_matrix = np.corrcoef(X_train_no_nans.T)\n",
    "\n",
    "    r, c = np.where(np.isnan(X_test))\n",
    "\n",
    "    rows_with_nan, rows_index, rows_counts = np.unique(r, return_index=True, return_counts=True)\n",
    "\n",
    "    for row, idx, counts in zip(rows_with_nan, rows_index, rows_counts):\n",
    "\n",
    "        missing_columns = c[idx:idx+counts]\n",
    "\n",
    "        corr_rows = corr_matrix[missing_columns]\n",
    "\n",
    "        # Set correlation to itselve and other missing features to zero \n",
    "        corr_rows[:, missing_columns] = 0\n",
    "\n",
    "        # Take absolute from correlations and sort row ascendingly\n",
    "        abs_corr_rows = np.abs(corr_rows)\n",
    "        sorted_corr_indices = np.argsort(abs_corr_rows)\n",
    "\n",
    "        n_corrs = n_features - len(missing_columns)\n",
    "\n",
    "        n_t = 8\n",
    "        n_c = 5\n",
    "\n",
    "        if n_corrs < n_c:\n",
    "            n_c = n_corrs\n",
    "            n_t = n_corrs\n",
    "        elif n_corrs < n_t:\n",
    "            n_t = n_corrs\n",
    "\n",
    "        for column_index in range(len(missing_columns)):\n",
    "            \n",
    "            missing_column = missing_columns[column_index] \n",
    "\n",
    "            # Randomly pick 5 out of 8 features with highest correlations\n",
    "            chosen_indices = np.random.choice(sorted_corr_indices[column_index, -n_t:], n_c, replace=False)\n",
    "\n",
    "            # Perform GLM on these 5 features to predict missing feature using the complete (no NaN) dataset\n",
    "            X = X_train_no_nans[:, chosen_indices]\n",
    "            y = X_train_no_nans[:, missing_column]\n",
    "\n",
    "            reg = linear_model.LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "\n",
    "            X_ = X_test[row, chosen_indices].reshape(1, -1)\n",
    "            y_ = reg.predict(X_)\n",
    "\n",
    "            X_test_imputed[row, missing_column] = np.squeeze(y_)\n",
    "\n",
    "    return X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imputation(X_with_nans_test, strategy):\n",
    "    \n",
    "    if strategy==\"mean\":\n",
    "        mean_imputer = Imputer(missing_values='NaN', strategy=\"mean\", axis=0)\n",
    "        X_imputed = mean_imputer.fit_transform(X_without_nans_train)\n",
    "        X_test_imputed = mean_imputer.transform(X_with_nans_test)\n",
    "        \n",
    "    if strategy==\"median\":\n",
    "        median_imputer = Imputer(missing_values='NaN', strategy=\"median\", axis=0)\n",
    "        X_imputed = median_imputer.fit_transform(X_without_nans_train)\n",
    "        X_test_imputed = median_imputer.transform(X_with_nans_test)\n",
    "    \n",
    "    if strategy==\"regression\":\n",
    "        X_test_imputed = impute_regression(X_without_nans_train, X_with_nans_test)\n",
    "        \n",
    "    if strategy==\"knn\":\n",
    "        \n",
    "        complete_id = range(X_with_nans_test.shape[0] + X_without_nans_train.shape[0])\n",
    "        train_id = list(set(complete_id) - set(test_id))\n",
    "        \n",
    "        X_to_impute = np.zeros((len(complete_id), X_without_nans_train.shape[1]))\n",
    "        X_to_impute[train_id] = X_without_nans_train\n",
    "        X_to_impute[test_id] = X_with_nans_test\n",
    "        \n",
    "        X_imputed = KNN(k=5, verbose=False).complete(X_to_impute)\n",
    "        X_test_imputed = X_imputed[test_id]\n",
    "    \n",
    "    return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('simulated_nans_data.pickle', 'rb') as handle:\n",
    "#     bool_nan_ind_container = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_nan_ind_0 = bool_nan_ind_container[-1, -1]\n",
    "# bool_nan_ind_0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['mean', 'median', 'regression', 'knn']\n",
    "n_splits = 10\n",
    "\n",
    "skfg = StratifiedKFoldByGroups(n_splits=n_splits) \n",
    "\n",
    "i = 0\n",
    "total_iterations = len(strategies) * fracs.size * n_resample * n_splits\n",
    "\n",
    "masked_MAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "masked_MSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "masked_RMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "masked_NMAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "masked_NMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "masked_NRMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "MAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "MSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "RMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "NMAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "NMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "NRMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "for i_f in range(len(fracs)):\n",
    "\n",
    "    # Create array to keep track of the amount of NaNs per column over resampling iterations\n",
    "    nan_distribution_per_f = np.zeros((X_true.shape[1]))\n",
    "    \n",
    "    t1 = time()\n",
    "    \n",
    "    for i_n in range(n_resample):\n",
    "        \n",
    "        bool_nan_ind = bool_nan_ind_container[i_f, i_n]\n",
    "        \n",
    "        X_sim_nans = X_true.copy()\n",
    "        X_sim_nans[bool_nan_ind] = np.nan\n",
    "        \n",
    "        nans_per_row = bool_nan_ind.sum(axis=1)\n",
    "        \n",
    "        nan_distribution_per_f = nan_distribution_per_f + bool_nan_ind.sum(axis=0)\n",
    "        \n",
    "        # Apply KFold (stratified for amount of NaNs per subject) to fit imputer on training data and assess it's performance on test data with NaN\n",
    "        for id_iter_cv, (train_id, test_id) in enumerate(skfg.split(X_sim_nans, np.zeros_like(nans_per_row), nans_per_row)):\n",
    "                    \n",
    "            X_sim_nans_test = X_sim_nans[test_id]\n",
    "            X_true_train, X_true_test =  X_true[train_id], X_true[test_id]\n",
    "            \n",
    "            bool_nan_ind_test = bool_nan_ind[test_id]\n",
    "                \n",
    "            for id_strategy, strategy in enumerate(strategies):\n",
    "                \n",
    "                # TODO: Pass X true \n",
    "                X_mean_imp_test = run_imputation(X_true_train, X_sim_nans_test, strategy, test_id)\n",
    "                    \n",
    "                # Mask all non imputed values with NaNs\n",
    "                X_true_test_masked, X_mean_imp_test_masked = X_true_test.copy(), X_mean_imp_test.copy()\n",
    "                X_true_test_masked[~bool_nan_ind_test], X_mean_imp_test_masked[~bool_nan_ind_test] = np.nan, np.nan\n",
    "                \n",
    "                # Suppress the warnings in this block\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "                    MAE_per_column = np.nanmean(np.absolute(X_true_test_masked  - X_mean_imp_test_masked), axis=0)\n",
    "                    MSE_per_column = np.nanmean(((X_true_test_masked - X_mean_imp_test_masked) ** 2), axis=0) \n",
    "                    RMSE_per_column = np.sqrt(MSE_per_column)  \n",
    "\n",
    "                    MAE_masked = np.nanmean(MAE_per_column) \n",
    "                    MSE_masked = np.nanmean(MSE_per_column) \n",
    "                    RMSE_masked = np.nanmean(RMSE_per_column) \n",
    "\n",
    "                    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4895707/\n",
    "                    NMAE_masked = np.nanmean(MAE_per_column / np.std(X_true_train, axis=0)) \n",
    "                    NMSE_masked = np.nanmean(MSE_per_column / np.std(X_true_train, axis=0)) \n",
    "                    NRMSE_masked = np.nanmean(RMSE_per_column / np.std(X_true_train, axis=0)) \n",
    "\n",
    "                    MAE = np.mean(np.mean(np.absolute(X_mean_imp_test - X_true_test), axis=0))\n",
    "                    MSE = np.mean(np.mean((X_mean_imp_test - X_true_test)**2, axis=0))\n",
    "                    RMSE = np.mean(np.sqrt(np.mean((X_mean_imp_test - X_true_test)**2, axis=0)))\n",
    "\n",
    "                    NMAE = np.mean(np.mean(np.absolute(X_mean_imp_test - X_true_test), axis=0) / np.std(X_true_train, axis=0))\n",
    "                    NMSE = np.mean(np.mean((X_mean_imp_test - X_true_test)**2, axis=0) / np.std(X_true_train, axis=0))\n",
    "                    NRMSE = np.mean(np.sqrt(np.mean((X_mean_imp_test - X_true_test)**2, axis=0)) / np.std(X_true_train, axis=0))\n",
    "\n",
    "                masked_MAE_scores[id_strategy, i_f, i_n, id_iter_cv] = MAE_masked\n",
    "                masked_MSE_scores[id_strategy, i_f, i_n, id_iter_cv] = MSE_masked\n",
    "                masked_RMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = RMSE_masked\n",
    "                \n",
    "                masked_NMAE_scores[id_strategy, i_f, i_n, id_iter_cv] = NMAE_masked\n",
    "                masked_NMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = NMSE_masked\n",
    "                masked_NRMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = NRMSE_masked\n",
    "                \n",
    "                MAE_scores[id_strategy, i_f, i_n, id_iter_cv] = MAE\n",
    "                MSE_scores[id_strategy, i_f, i_n, id_iter_cv] = MSE\n",
    "                RMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = RMSE\n",
    "                \n",
    "                NMAE_scores[id_strategy, i_f, i_n, id_iter_cv] = NMAE\n",
    "                NMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = NMSE\n",
    "                NRMSE_scores[id_strategy, i_f, i_n, id_iter_cv] = NRMSE\n",
    "\n",
    "                i += 1\n",
    "    t2 = time()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"Finished calculating similarity between original and imputed data for {}% missing data \\n Iteration: ({}/{}) completed in {} s \\n\".format(fracs[i_f]*100, i, total_iterations, t2 - t1))\n",
    "    perc_nan_col = (nan_distribution_per_f / float(nan_distribution_per_f.sum())) * 100    \n",
    "    print(\"{} % of induced NaNs in subcortical features, true percentage  = {} %\".format(perc_nan_col[:cort_surf_idx].sum(), true_perc_nan_subc))\n",
    "    print(\"{} % of induced NaNs in cortical surface area features, true percentage  = {} %\".format(perc_nan_col[cort_surf_idx:cort_thick_idx].sum(), true_perc_nan_cort_s)) \n",
    "    print(\"{} % of induced NaNs in cortical thickness features, true percentage  = {} %\".format(perc_nan_col[cort_thick_idx:].sum(), true_perc_nan_cort_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [masked_MAE_scores, masked_MSE_scores, masked_RMSE_scores, \n",
    "         masked_NMAE_scores, masked_NMSE_scores, masked_NRMSE_scores,\n",
    "         MAE_scores, MSE_scores, RMSE_scores,\n",
    "         NMAE_scores, NMSE_scores, NRMSE_scores]\n",
    "\n",
    "filenames = ['masked_MAE_scores', 'masked_MSE_scores', 'masked_RMSE_scores', \n",
    "             'masked_NMAE_scores', 'masked_NMSE_scores', 'masked_NRMSE_scores',\n",
    "             'MAE_scores', 'MSE_scores', 'RMSE_scores',\n",
    "             'NMAE_scores', 'NMSE_scores', 'NRMSE_scores']\n",
    "\n",
    "for f, fname in zip(files, filenames):\n",
    "    with open((fname + '.pickle'), 'wb') as handle:\n",
    "        pickle.dump(f, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategies = ['mean', 'median', 'regression', 'knn']\n",
    "# n_splits = 10\n",
    "\n",
    "# masked_MAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# masked_MSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# masked_RMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "# masked_NMAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# masked_NMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# masked_NRMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "# MAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# MSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# RMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "# NMAE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# NMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "# NRMSE_scores = np.zeros((len(strategies), fracs.size, n_resample, n_splits))\n",
    "\n",
    "\n",
    "# files = [masked_MAE_scores, masked_MSE_scores, masked_RMSE_scores, \n",
    "#          masked_NMAE_scores, masked_NMSE_scores, masked_NRMSE_scores,\n",
    "#          MAE_scores, MSE_scores, RMSE_scores,\n",
    "#          NMAE_scores, NMSE_scores, NRMSE_scores]\n",
    "\n",
    "# filenames = ['masked_MAE_scores', 'masked_MSE_scores', 'masked_RMSE_scores', \n",
    "#              'masked_NMAE_scores', 'masked_NMSE_scores', 'masked_NRMSE_scores',\n",
    "#              'MAE_scores', 'MSE_scores', 'RMSE_scores',\n",
    "#              'NMAE_scores', 'NMSE_scores', 'NRMSE_scores']\n",
    "\n",
    "# for i, (f, fname) in enumerate(zip(files, filenames)):\n",
    "#     with open(fname +'.pickle', 'rb') as handle:\n",
    "#         data = pickle.load(handle)\n",
    "#         files[i] = data\n",
    "#         print(\"Finished loading {}\".format(fname))\n",
    "        \n",
    "        \n",
    "# masked_MAE_scores, masked_MSE_scores, masked_RMSE_scores, masked_NMAE_scores, masked_NMSE_scores, masked_NRMSE_scores, MAE_scores, MSE_scores, RMSE_scores, NMAE_scores, NMSE_scores, NRMSE_scores = files[0],  files[1],  files[2],  files[3],  files[4],  files[5],  files[6],  files[7],  files[8],  files[9],  files[10],  files[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.mean(masked_NRMSE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.mean(masked_NRMSE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.min(data_mean - data_sd)\n",
    "y_max = np.max(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('masked NRMSE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "    \n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(NRMSE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(NRMSE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('NRMSE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy, alpha=0.9)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "    \n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(masked_MAE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(masked_MAE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('masked MAE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "\n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(MAE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(MAE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('MAE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "\n",
    "    \n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(masked_MAE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(masked_MAE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('NMAE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "\n",
    "    \n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(masked_NMSE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(masked_NMSE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('MSE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "\n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['r', 'b', 'g', 'y']\n",
    "\n",
    "data_mean = np.nanmean(NMSE_scores, axis=3).mean(axis=2)\n",
    "data_sd = np.nanmean(NMSE_scores, axis=3).std(axis=2)\n",
    "\n",
    "y_min = np.nanmin(data_mean - data_sd)\n",
    "y_max = np.nanmax(data_mean + data_sd)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylabel('MSE scores', size='x-large')\n",
    "plt.ylim([y_min, y_max])\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "\n",
    "    plt.plot(fracs, data_mean[i, :], color_palette[i], label=strategy)\n",
    "\n",
    "    plt.axvline(x=missing_data_percentage, color='black', ls='dashed', alpha=0.5)\n",
    "    plt.fill_between(fracs, data_mean[i, :] - data_sd[i, :], data_mean[i, :] + data_sd[i, :], color=color_palette[i], alpha=0.1)\n",
    "\n",
    "ticks = np.arange(0, max(fracs) + 0.05, 0.05)\n",
    "labels = [str(int(tick * 100)) for tick in ticks]\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.xlabel('Missing data %', size='x-large')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_percentage * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
