{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import os \n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "\n",
    "#from modshogun import *\n",
    "\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, LeaveOneGroupOut\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, Imputer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from CustomCVs import KFoldMixedSizes, StratifiedKFoldMixedSizes, StratifiedKFoldByGroups\n",
    "#from evaluation_classifier import Evaluater\n",
    "\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute, IterativeSVD #, MICE\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_rank_k_dataset(\n",
    "#         n_rows=5,\n",
    "#         n_cols=5,\n",
    "#         k=3,\n",
    "#         fraction_missing=0.1,\n",
    "#         symmetric=False,\n",
    "#         random_seed=0):\n",
    "#     np.random.seed(random_seed)\n",
    "#     x = np.random.randn(n_rows, k)\n",
    "#     y = np.random.randn(k, n_cols)\n",
    "\n",
    "#     XY = np.dot(x, y)\n",
    "\n",
    "#     if symmetric:\n",
    "#         assert n_rows == n_cols\n",
    "#         XY = 0.5 * XY + 0.5 * XY.T\n",
    "\n",
    "#     missing_raw_values = np.random.uniform(0, 1, (n_rows, n_cols))\n",
    "#     missing_mask = missing_raw_values < fraction_missing\n",
    "\n",
    "#     XY_incomplete = XY.copy()\n",
    "#     # fill missing entries with NaN\n",
    "#     XY_incomplete[missing_mask] = np.nan\n",
    "\n",
    "#     return XY, XY_incomplete, missing_mask\n",
    "\n",
    "# # create some default data to be shared across tests\n",
    "# XY, XY_incomplete, missing_mask = create_rank_k_dataset(\n",
    "#     n_rows=500,\n",
    "#     n_cols=10,\n",
    "#     k=3,\n",
    "#     fraction_missing=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlated_dataset(cov_mat, var_vec, mean_vec, n_obs = 2500):\n",
    "    \n",
    "    n_vars = cov_mat.shape[0]\n",
    "    cov_mat = cov_mat + 0.5 * np.eye(n_vars) # regularize for stability\n",
    "\n",
    "    FUDGE_var = 1.5\n",
    "    FUDGE_mean = 0.6\n",
    "    try:\n",
    "        L = np.linalg.cholesky(cov_mat)\n",
    "        D = np.dot(L, np.random.uniform(0,1, (n_vars, n_obs)))\n",
    "        D = FUDGE_var*D*var_vec.T + FUDGE_mean*mean_vec.T # get the scaling right\n",
    "        return D\n",
    "    \n",
    "    except np.linalg.LinAlgError as err:\n",
    "        print('Error ---- Cholesksy')\n",
    "        return None\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.47571974, 0.49593642, 0.34144643],\n",
       "       [0.47571974, 1.        , 0.62849577, 0.18159635],\n",
       "       [0.49593642, 0.62849577, 1.        , 0.206955  ],\n",
       "       [0.34144643, 0.18159635, 0.206955  , 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "cov_mat = np.array([[1, 0.7, 0.7, 0.5,],\n",
    "             [0.7, 1, 0.95, 0.3],\n",
    "             [0.7, 0.95, 1, 0.3],\n",
    "             [0.5, 0.3, 0.3, 1]])\n",
    "mean_vec=np.array([200, 200, 200, 200], ndmin=2)\n",
    "var_vec=np.array([100, 100, 100, 100], ndmin=2)\n",
    "\n",
    "D = create_correlated_dataset(cov_mat, var_vec, mean_vec)\n",
    "print(D.shape)\n",
    "np.corrcoef(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123.55709463, 157.80295564, 245.21950185, ..., 128.2970865 ,\n",
       "        273.92699658, 196.19967885],\n",
       "       [134.63689691, 180.32076221, 294.06000022, ..., 215.31309286,\n",
       "        250.73332266, 235.03182177],\n",
       "       [147.66855353, 247.30038001, 309.60694575, ..., 304.11095134,\n",
       "        333.11447241, 303.25131543],\n",
       "       [228.07998155, 171.02888621, 335.41302081, ..., 268.04953838,\n",
       "        288.95218998, 177.66750908]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a cov matrix using a REAL dataset\n",
    "\n",
    "data_dir=\"/data/rmthomas/HeteroSmallSample\"\n",
    "df = pd.read_csv(os.path.join(data_dir, \"real_data.csv\"))\n",
    "df_numeric = df[df.columns[25:125]]\n",
    "\n",
    "# cov_mat_overall = df_numeric.corr().values\n",
    "# alpha = 0.2\n",
    "# reg_cov_mat = cov_mat_overall + alpha*np.eye(100) # alpha makes the matrix well conditioned for Cholesky\n",
    "# D_overall = create_correlated_dataset(reg_cov_mat)\n",
    "# print(D_overall.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100 \n",
    "g = df.groupby(['site', 'Dx', 'age_group'])[df.columns[25:25+n_features]]\n",
    "Groups = list(g.indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation matrices per set = (site, Dx, age_group)\n",
    "corrs_per_set = g.corr().values.reshape(-1, n_features, n_features)\n",
    "means_per_set = g.mean().values.reshape(-1, n_features)\n",
    "vars_per_set = g.std().values.reshape(-1, n_features)\n",
    "#corrs_per_set[np.where(np.isnan(corrs_per_set))] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = [f'f{i}' for i in range(100)] # f1, f2 ...f100\n",
    "data_cols = ['site', 'Dx', 'Age_group'] + feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n",
      "Error ---- Cholesksy\n"
     ]
    }
   ],
   "source": [
    "sim_data_all = pd.DataFrame(columns=data_cols) # initialize a dataframe\n",
    "group_template = pd.DataFrame(columns=['site', 'Dx', 'Age_group']) # initialize a dataframe\n",
    "\n",
    "sim_data=[]\n",
    "\n",
    "min_subj = 20\n",
    "max_subj = 120\n",
    "for corr_i in range(corrs_per_set.shape[0]):\n",
    "    n_obs=np.random.choice(np.arange(min_subj, max_subj))\n",
    "    D = create_correlated_dataset(corrs_per_set[corr_i], vars_per_set[corr_i].reshape(1,n_features), means_per_set[corr_i].reshape(1,n_features), n_obs=n_obs)\n",
    "\n",
    "    if D is not None:\n",
    "        sim_data_group = pd.DataFrame([list(Groups[corr_i])], columns=['site', 'Dx', 'Age_group'])\n",
    "        sim_data_group = pd.concat([sim_data_group]*n_obs, ignore_index=True)\n",
    "        \n",
    "        sim_data_group_matrix = pd.DataFrame(D.T, columns=feature_labels)\n",
    "        df_group = pd.concat([sim_data_group, sim_data_group_matrix], axis=1, ignore_index=False)\n",
    "        sim_data_all = sim_data_all.append(df_group, ignore_index=True)\n",
    "        sim_data.append(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3638, 103)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.12295826e+03, 2.83240357e+03, 3.16672976e+03, 1.34195979e+03,\n",
       "        4.73346740e+03, 6.17493193e+03, 7.02186019e+03, 5.35349764e+02,\n",
       "        1.38686473e+03, 3.32656379e+03, 3.46925039e+03, 1.43302906e+03,\n",
       "        4.96038813e+03, 6.24378893e+03, 6.99421012e+04, 9.51810812e+02,\n",
       "        6.21513058e+02, 2.18448349e+03, 1.26703705e+03, 3.85857865e+02,\n",
       "        1.87746170e+02, 3.00089642e+03, 4.56014032e+03, 3.26823609e+03,\n",
       "        1.91708514e+03, 1.03625930e+03, 4.45436848e+03, 2.34049795e+03,\n",
       "        2.84253832e+03, 1.75409821e+03, 3.05234714e+03, 1.24079370e+03,\n",
       "        6.97006721e+02, 1.64250054e+03, 6.07061063e+02, 1.30044808e+03,\n",
       "        1.32656291e+03, 4.02982578e+03, 1.20983145e+03, 4.50275296e+03,\n",
       "        3.79082297e+03, 9.50054348e+02, 6.03807578e+03, 7.14043739e+03,\n",
       "        5.30195166e+03, 3.73371973e+03, 4.13926531e+03, 4.33316441e+02,\n",
       "        4.92652777e+02, 8.49666950e+04, 9.75166262e+02, 8.61015105e+02,\n",
       "        2.39301123e+03, 1.55501280e+03, 3.67452821e+02, 2.74456123e+02,\n",
       "        3.33437897e+03, 5.90623045e+03, 3.48649958e+03, 2.16539390e+03,\n",
       "        1.05150273e+03, 4.74479856e+03, 2.55993517e+03, 3.01273725e+03,\n",
       "        1.81551119e+03, 3.63759187e+03, 1.52632639e+03, 7.27843522e+02,\n",
       "        1.49332248e+03, 7.90654515e+02, 1.65651037e+03, 1.58886108e+03,\n",
       "        4.14331444e+03, 1.38581131e+03, 4.84185822e+03, 4.26842139e+03,\n",
       "        7.91606885e+02, 6.51471729e+03, 7.32266352e+03, 5.60364110e+03,\n",
       "        3.61582986e+03, 4.04676197e+03, 3.83163070e+02, 3.69113000e+02,\n",
       "        1.50935530e+00, 1.60930391e+00, 1.59985287e+00, 1.64360156e+00,\n",
       "        1.21617015e+00, 2.16104075e+00, 1.83861538e+00, 1.73997090e+00,\n",
       "        1.56541786e+00, 1.84848650e+00, 2.03361772e+00, 1.67444281e+00,\n",
       "        1.40947364e+00, 1.75605973e+00, 1.38329913e+00, 1.65603245e+00]),\n",
       " array([1.56012177e+03, 3.74339947e+03, 4.24876388e+03, 1.60682119e+03,\n",
       "        5.83181948e+03, 7.94735520e+03, 6.07851601e+03, 6.14649585e+02,\n",
       "        1.64741114e+03, 3.79278601e+03, 4.33262128e+03, 1.59768111e+03,\n",
       "        5.64914946e+03, 7.32613708e+03, 8.48732576e+04, 1.03846665e+03,\n",
       "        6.38001932e+02, 2.32004666e+03, 1.46175845e+03, 4.04456366e+02,\n",
       "        2.11954059e+02, 3.27936431e+03, 4.72735186e+03, 3.33696402e+03,\n",
       "        2.14761466e+03, 1.00877248e+03, 4.82362320e+03, 2.57959011e+03,\n",
       "        3.05522965e+03, 1.81330053e+03, 3.08811222e+03, 1.33563782e+03,\n",
       "        7.21324784e+02, 1.67122501e+03, 6.25391895e+02, 1.29392593e+03,\n",
       "        1.35246502e+03, 4.23394213e+03, 1.18186549e+03, 4.85345503e+03,\n",
       "        3.82644609e+03, 8.05232014e+02, 5.71354033e+03, 7.17764588e+03,\n",
       "        5.48025734e+03, 3.74338365e+03, 3.93545747e+03, 4.80041955e+02,\n",
       "        4.63590336e+02, 8.52443896e+04, 9.56627541e+02, 7.64960827e+02,\n",
       "        2.14259334e+03, 1.51880478e+03, 3.44142322e+02, 2.87086346e+02,\n",
       "        3.18447916e+03, 5.54875282e+03, 3.18267871e+03, 2.19146076e+03,\n",
       "        9.50822408e+02, 4.68387767e+03, 2.55923092e+03, 3.06351915e+03,\n",
       "        1.78791244e+03, 3.41868109e+03, 1.52154130e+03, 6.91019923e+02,\n",
       "        1.40091158e+03, 7.70808187e+02, 1.50208612e+03, 1.48958620e+03,\n",
       "        4.07055867e+03, 1.21157027e+03, 4.88358706e+03, 3.99548832e+03,\n",
       "        6.62519189e+02, 5.90694866e+03, 6.94169773e+03, 5.47246180e+03,\n",
       "        3.58327681e+03, 3.74020046e+03, 4.21498198e+02, 3.46857584e+02,\n",
       "        2.49599496e+00, 2.49762085e+00, 2.73348829e+00, 2.55705049e+00,\n",
       "        1.86672374e+00, 3.25047780e+00, 2.88154225e+00, 2.70127880e+00,\n",
       "        2.45520863e+00, 2.78733131e+00, 3.08400750e+00, 2.55618221e+00,\n",
       "        2.17439625e+00, 2.68975511e+00, 2.03651241e+00, 2.50536789e+00])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sim_data_all.mean().values, df_numeric.mean().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to file\n",
    "sim_data_all.to_csv('simulated_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.hstack(sim_data)\n",
    "nvars, nsubjs = all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for each group\n",
    "D_patients = create_correlated_dataset(cov_patients)\n",
    "D_controls = create_correlated_dataset(cov_controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(np.array([1,1,0]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,3])\n",
    "out = (np.arange(4) == a[:,None]).astype(np.float32)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(4)[None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(4)[None,:] == a[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
